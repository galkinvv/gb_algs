/**\file
Пересылка подматриц.
Процедуры широковещательной (основанной на MPI_Bcast) и попарной (MPI_Send/MPI_Recv) пересылки.
Процедуры отправки принимают два итератора - на начало и на конец набора строк.
Поскольку размер получаемого объекта неизвестен на момент вызова процедуры приёма,
в качестве аргументов она принимает не пару итераторов, а ссылку на матрицу,
в которую дописываются полученные строки.
Возвращаемое значение равно числу полученных строк.
Значения useBigSends у отправителя и получателя в пределах одной пересылки должны совпадать.
*/
namespace F4MPI{
/**
Отправка подматрицы.
Отправляет на процессор \a recvID подматрицу, составленную из строк [\a from, \a to) 
\param recvID MPI-ранг процесса-получателя (относительно MPI_COMM_WORLD)
\param from итератор, указывающий на первую строку подматрицы
\param to итератор, указывающий за последнюю строку подматрицы
\param useBigSends установка в \c true указывает,
что нужно минимизировать число отдельнхых пересылок ценой лишнего копирования всех данных в(из) единую связную область.
Тогда используеися только 2 пересылки - пересылка общего размера данных и, собственно, самих данных.
*/
template <class MatrixIterator>
void sendSubMatrix(int recvID, MatrixIterator from,MatrixIterator to, bool useBigSends);

/**
Получение подматрицы.
Получает с процессора \a senderID подматрицу, и дописывает строки в \a m
\param senderID MPI-ранг процесса-отправителя (относительно MPI_COMM_WORLD)
\param m матрица, в которую дописываются полученные строки
\param useBigSends установка в \c true указывает,
что нужно минимизировать число отдельнхых пересылок ценой лишнего копирования всех данных в(из) единую связную область.
Тогда используеися только 2 пересылки - пересылка общего размера данных и, собственно, самих данных.
\retval число полученных строк
*/
template <class CMatrix>
int recvToMatrix(int senderID, CMatrix& m, bool useBigSends);

/**
Широковещательная отправка подматрицы.
Рассылает всем процессам входящим в MPI-коммуникатор \a comm подматрицу, составленную из строк [\a from, \a to) 
\param senderID MPI-ранг процесса-отправителя в коммуникаторе \a comm
\param from итератор, указывающий на первую строку подматрицы
\param to итератор, указывающий за последнюю строку подматрицы
\param comm MPI-коммуникатор, описывающий множество процессов по которому происходит рассылка
\param useBigSends установка в \c true указывает,
что нужно минимизировать число отдельнхых пересылок ценой лишнего копирования всех данных в(из) единую связную область.
Тогда используеися только 2 пересылки - пересылка общего размера данных и, собственно, самих данных.
*/
template <class MatrixIterator, class MPI_Comm>
void bcastSendSubMatrix(int senderID, MatrixIterator from, MatrixIterator to, MPI_Comm comm, bool useBigSends);

/**
Широковещательное получение подматрицы.
Получает отправленную процессом \a senderID всем процессам входящим в MPI-коммуникатор \a comm подматрицу,
и дописывает строки в \a m
\param senderID MPI-ранг процесса-отправителя в коммуникаторе comm
\param m матрица, в которую дописываются полученные строки
\param comm MPI-коммуникатор, описывающий множество процессов по которому происходит рассылка
\param useBigSends установка в \c true указывает,
что нужно минимизировать число отдельнхых пересылок ценой лишнего копирования всех данных в(из) единую связную область.
Тогда используеися только 2 пересылки - пересылка общего размера данных и, собственно, самих данных.
*/
template <class CMatrix, class MPI_Comm>
int bcastRecvToMatrix(int senderID, CMatrix& m, MPI_Comm comm, bool useBigSends);

/**
Широковещательная передача матрицы.
Рассылает матрицу \a m с процессора \a senderID всем процессам входящим в MPI-коммуникатор \a comm и записывает её на них в \a m.
В результате, после вызова процедуры матрица \a m становится одинаковой во всех процессах.
\param senderID MPI-ранг процесса-отправителя в коммуникаторе \a comm
\param m матрица, для которой проводится передача
\param myID MPI-ранг данного порцесса (процесса, вызывающего процедуру) в коммуникаторе \a comm
\param comm MPI-коммуникатор, описывающий множество процессов по которому происходит рассылка
\param useBigSends установка в \c true указывает,
что нужно минимизировать число отдельнхых пересылок ценой лишнего копирования всех данных в(из) единую связную область.
Тогда используеися только 2 пересылки - пересылка общего размера данных и, собственно, самих данных.
*/
template <class CMatrix, class MPI_Comm>
void bcastMat(int senderID, CMatrix& m, int myID, MPI_Comm comm, bool useBigSends);
} //namespace F4MPI
